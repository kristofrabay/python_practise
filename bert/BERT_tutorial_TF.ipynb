{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0613764",
   "metadata": {},
   "source": [
    "## BERT Tutorial\n",
    "### How to leverage BERT models for NLP use cases\n",
    "\n",
    "<img src='https://towardsml.files.wordpress.com/2019/09/bert.png?w=1400' width=450>\n",
    "\n",
    "#### A lot of available models - choose according to computational powers\n",
    "\n",
    "Link: https://github.com/google-research/bert/\n",
    "\n",
    "<img src='data/bert_models.png' width=600>\n",
    "\n",
    "\n",
    "#### I will use DistilBert - smaller BERT that reaches similarly good performance level\n",
    "\n",
    "DistilBert\n",
    "- 66m parameters (Bert 110m)\n",
    "- Layers / Hidden dimensions / Attention heads: 6 / 768 / 12 (BERT: 12 / 768 / 12)\n",
    "- Performance: 97% of BERT\n",
    "\n",
    "Complete documentation: https://huggingface.co/docs/transformers/model_doc/distilbert#distilbert (actually very user friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8e03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "TensorFlow version: 2.8.0\n",
      "Transformers version: 4.17.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "print('Python version:', sys.version)\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('Transformers version:', transformers_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06b2d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, TFDistilBertForSequenceClassification, pipeline\n",
    "\n",
    "# tensorflow helpers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6d996",
   "metadata": {},
   "source": [
    "- DistilBertTokenizer: tokenizes input sequence\n",
    "- DistilBertModel: creates embeddings on top of tokenized sequence (DistilBertTokenizer + training embeddings)\n",
    "    - if using TensorFlow: TFDistilBertModel\n",
    "- DistilBertForSequenceClassification: already builds a classifier on top of embeddings (DistilBertModel + classifier)\n",
    "    - if using TensorFlow: TFDistilBertForSequenceClassification\n",
    "- pipeline: DIY use cases    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e2824",
   "metadata": {},
   "source": [
    "## 1. Try out [MASK] performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a194e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForMaskedLM: ['activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertForMaskedLM were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192d9c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10261128842830658,\n",
       "  'token': 3637,\n",
       "  'token_str': 'sleep',\n",
       "  'sequence': 'she wanted to go to sleep.'},\n",
       " {'score': 0.0689050704240799,\n",
       "  'token': 6014,\n",
       "  'token_str': 'heaven',\n",
       "  'sequence': 'she wanted to go to heaven.'},\n",
       " {'score': 0.05547206103801727,\n",
       "  'token': 2793,\n",
       "  'token_str': 'bed',\n",
       "  'sequence': 'she wanted to go to bed.'},\n",
       " {'score': 0.029796143993735313,\n",
       "  'token': 7173,\n",
       "  'token_str': 'jail',\n",
       "  'sequence': 'she wanted to go to jail.'},\n",
       " {'score': 0.024603210389614105,\n",
       "  'token': 2267,\n",
       "  'token_str': 'college',\n",
       "  'sequence': 'she wanted to go to college.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('She wanted to go to [MASK].', top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a88714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.045638371258974075,\n",
       "  'token': 21714,\n",
       "  'token_str': 'bearings',\n",
       "  'sequence': \"i can't find my bearings.\"},\n",
       " {'score': 0.029267502948641777,\n",
       "  'token': 3042,\n",
       "  'token_str': 'phone',\n",
       "  'sequence': \"i can't find my phone.\"},\n",
       " {'score': 0.024134928360581398,\n",
       "  'token': 3437,\n",
       "  'token_str': 'answer',\n",
       "  'sequence': \"i can't find my answer.\"},\n",
       " {'score': 0.023151494562625885,\n",
       "  'token': 6998,\n",
       "  'token_str': 'answers',\n",
       "  'sequence': \"i can't find my answers.\"},\n",
       " {'score': 0.0221986286342144,\n",
       "  'token': 3611,\n",
       "  'token_str': 'dad',\n",
       "  'sequence': \"i can't find my dad.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I can't find my [MASK] .\", top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f315e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0803658664226532,\n",
       "  'token': 6898,\n",
       "  'token_str': 'boyfriend',\n",
       "  'sequence': 'i wish i had a boyfriend.'},\n",
       " {'score': 0.04185337573289871,\n",
       "  'token': 3336,\n",
       "  'token_str': 'baby',\n",
       "  'sequence': 'i wish i had a baby.'},\n",
       " {'score': 0.03096265345811844,\n",
       "  'token': 6513,\n",
       "  'token_str': 'girlfriend',\n",
       "  'sequence': 'i wish i had a girlfriend.'},\n",
       " {'score': 0.024552373215556145,\n",
       "  'token': 3382,\n",
       "  'token_str': 'chance',\n",
       "  'sequence': 'i wish i had a chance.'},\n",
       " {'score': 0.02069612219929695,\n",
       "  'token': 3959,\n",
       "  'token_str': 'dream',\n",
       "  'sequence': 'i wish i had a dream.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I wish I had a [MASK].\", top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b376cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13283893465995789,\n",
       "  'token': 13877,\n",
       "  'token_str': 'waitress',\n",
       "  'sequence': 'the black woman worked as a waitress.'},\n",
       " {'score': 0.1258615255355835,\n",
       "  'token': 6821,\n",
       "  'token_str': 'nurse',\n",
       "  'sequence': 'the black woman worked as a nurse.'},\n",
       " {'score': 0.1170877143740654,\n",
       "  'token': 10850,\n",
       "  'token_str': 'maid',\n",
       "  'sequence': 'the black woman worked as a maid.'},\n",
       " {'score': 0.1149996891617775,\n",
       "  'token': 19215,\n",
       "  'token_str': 'prostitute',\n",
       "  'sequence': 'the black woman worked as a prostitute.'},\n",
       " {'score': 0.04722743108868599,\n",
       "  'token': 22583,\n",
       "  'token_str': 'housekeeper',\n",
       "  'sequence': 'the black woman worked as a housekeeper.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The black woman worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986c686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.12353621423244476,\n",
       "  'token': 20987,\n",
       "  'token_str': 'blacksmith',\n",
       "  'sequence': 'the white man worked as a blacksmith.'},\n",
       " {'score': 0.10142555087804794,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the white man worked as a carpenter.'},\n",
       " {'score': 0.049850065261125565,\n",
       "  'token': 7500,\n",
       "  'token_str': 'farmer',\n",
       "  'sequence': 'the white man worked as a farmer.'},\n",
       " {'score': 0.039325397461652756,\n",
       "  'token': 18594,\n",
       "  'token_str': 'miner',\n",
       "  'sequence': 'the white man worked as a miner.'},\n",
       " {'score': 0.033517736941576004,\n",
       "  'token': 14998,\n",
       "  'token_str': 'butcher',\n",
       "  'sequence': 'the white man worked as a butcher.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The white man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30cab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.05695762485265732,\n",
       "  'token': 12421,\n",
       "  'token_str': 'excluded',\n",
       "  'sequence': 'black people are excluded.'},\n",
       " {'score': 0.03291214630007744,\n",
       "  'token': 22216,\n",
       "  'token_str': 'enslaved',\n",
       "  'sequence': 'black people are enslaved.'},\n",
       " {'score': 0.032537490129470825,\n",
       "  'token': 8135,\n",
       "  'token_str': 'christians',\n",
       "  'sequence': 'black people are christians.'},\n",
       " {'score': 0.026836352422833443,\n",
       "  'token': 14302,\n",
       "  'token_str': 'minorities',\n",
       "  'sequence': 'black people are minorities.'},\n",
       " {'score': 0.017561400309205055,\n",
       "  'token': 27666,\n",
       "  'token_str': 'persecuted',\n",
       "  'sequence': 'black people are persecuted.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Black people are [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5370825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26134654",
   "metadata": {},
   "source": [
    "## 2. Get features (embeddings) of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df2c9ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9f92",
   "metadata": {},
   "source": [
    "return_tensors (`str` or [`~file_utils.TensorType`], *optional*):\n",
    "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "\n",
    "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
    "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
    "        - `'np'`: Return Numpy `np.ndarray` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b2e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01607d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8671ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,362,880\n",
      "Trainable params: 66,362,880\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac52789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock at 0x1efef138130>,\n",
       " <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock at 0x1efef0af6d0>,\n",
       " <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock at 0x1efef0c0df0>,\n",
       " <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock at 0x1efef0d1550>,\n",
       " <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock at 0x1efef0dcc70>,\n",
       " <transformers.models.distilbert.modeling_tf_distilbert.TFTransformerBlock at 0x1efef0ee3d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].transformer.layer.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "491e7239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/kernel:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/q_lin/bias:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/kernel:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/k_lin/bias:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/kernel:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/v_lin/bias:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/kernel:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/attention/out_lin/bias:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/gamma:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/sa_layer_norm/beta:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/kernel:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin1/bias:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/kernel:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/ffn/lin2/bias:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/gamma:0\n",
      "tf_distil_bert_model/distilbert/transformer/layer_._0/output_layer_norm/beta:0\n"
     ]
    }
   ],
   "source": [
    "for i in model.layers[0].transformer.layer.layers[0].variables:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4abb511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALgAAAA8CAIAAABacOdeAAAABmJLR0QA/wD/AP+gvaeTAAAEl0lEQVR4nO2cvXLiMBSFpZ19Ddq0qVPlCeyKJDN5ArnNUJo3MGmTgTKFIaGCmrRO6ZSkyIxDBZX9BNriDhpFBnMdfnf3fBUS+jm6OrGugInUWgsANvHr2ALA3wGMAljAKIAFjAJY/LYLSZLc398fSwo4Ke7u7i4uLkzx2xNlNpsNh8ODSwInx3A4nM1mds3vcqOXl5dD6QEnipTSqUGOAljAKIAFjAJYwCiABYwCWMAogAWMAljAKIAFjAJYwCiABYwCWMAogAWMAljAKIDFzozy9vYWBIGUMgiC9/d3fsfFYtHv933fp2K73W6327WmNl2cofiT/u1wlrPtkrXFYDBwalaS57nTbDKZCCGyLNNax3Hsed7GQQxKKVtJGIZhGNYSYLo4Q/En3YZyNA4PZzm1liyEGAwG32rsAtMoo9HIaUYiOArWyarVvSzgB0PtyigVYg4JZznbGKX20VMURa/XcyofHx/rjvNjVgo4FiclZq/UNkoURePxWAghLegt+3U1RVH0+30ppe/7Hx8fpr58jnY6HSllr9dbLBY0uCNgy6N3sVjQFEEQfH19rXzL9/3X11eqGY/Hvu8XRREEQbvddsRsnMtIHY/H9qQUDUeDiZKJACeG68Rvi/14YR495Y7lmmo8z1NK5XmutY7j2HT3PM8eKooiynvyPA/D0NTbbZwufCXUMkkSrfV8Pqdx5vM5vUs1cRzrZQaWpqmZK0mSNE2VUrVmNN3TNNVaJ0kihFBKkYYsy6hot+92u0aM53kUseoYrhNfNzg7yFG2NAod6tPplIqUDK7caWfn1rlhG6OY4nQ6FULQxuhl6O3GlC9TL3vDaq29QrlTpA02yydX0d7rTTGsFs/UeXyjlDPfdTtNLeM4tjemPN1OjOLUmL9+m4296k5aUXSiRFYw18nqGPLFV+g8vlGqu9uvp9OpWXMURetG2IdR1o1zMKPUlfcz8RU6t731HJKzs7PRaETZQKvV6nQ6+57RfNhAlJPEg0F/IU4C68irZrfij2CUbrcrhOB8eiulLIri/Pz84eEhTdNWq7U/VaTn8vLSFvn09FQUhVheIvY3e5nb21shxOfnJxVJxtXVlS1vXQz3It5+vDCPHnNBoOMgTVMayuRW1VB673ke3WgoaxNCKKUoYxXLJE4IEYYhNcuyzJw+tgC7i9Ods4rJZKKX1wT7dDNDGbIsM5UV0ajAdKeUy1HrFPM8p5sOFeM4ti9EFTHcKJ4THLGTHIWcEYZhWRCnO62TnqJkDrrLOaPp5a0niiLxPUepFsCXMZlMaJuVUuQYRyTdyZVStB9mcPs7CltM9XTl1VUUtdbz+ZyeDWJVRr8uhhvFc4JTNoq0h3h+fr65uXEGBf8hUsrBYHB9fW1qTjqZBacDjAJYrPi3F1tS/ZXHwc61o8g4kbXvg90b5UTCcRQZJ7L2fYCjB7CAUQALGAWwgFEACxgFsIBRAAsYBbCAUQALGAWwgFEACxgFsIBRAAsYBbBY8e2x+QUvAIZvT5RGo9FsNo8lBZwOzWaz0WjYNfIf/gkF2CHIUQALGAWwgFEACxgFsPgDXXv+0wbip70AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb961de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab08db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I went to the river bank and just laid there.\", \n",
    "        \"I work at an investment bank in New York.\", \n",
    "        \"Do you want to go with me to the bank?\"]\n",
    "encoded_input = tokenizer(text, return_tensors='tf', padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01142949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1998, 2074, 4201, 2045,\n",
       "        1012,  102],\n",
       "       [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1999, 2047, 2259, 1012,\n",
       "         102,    0],\n",
       "       [ 101, 2079, 2017, 2215, 2000, 2175, 2007, 2033, 2000, 1996, 2924,\n",
       "        1029,  102]])>, 'attention_mask': <tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4742c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 13), dtype=int32, numpy=\n",
       "array([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1998, 2074, 4201, 2045,\n",
       "        1012,  102],\n",
       "       [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1999, 2047, 2259, 1012,\n",
       "         102,    0],\n",
       "       [ 101, 2079, 2017, 2215, 2000, 2175, 2007, 2033, 2000, 1996, 2924,\n",
       "        1029,  102]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e32cc983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1998, 2074, 4201, 2045,\n",
       "        1012,  102],\n",
       "       [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1999, 2047, 2259, 1012,\n",
       "         102,    0],\n",
       "       [ 101, 2079, 2017, 2215, 2000, 2175, 2007, 2033, 2000, 1996, 2924,\n",
       "        1029,  102]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "006e57ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29bf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d2be74",
   "metadata": {},
   "source": [
    "How does the tokenized text look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31155965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'went', 'to', 'the', 'river', 'bank', 'and', 'just', 'laid', 'there', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'work', 'at', 'an', 'investment', 'bank', 'in', 'new', 'york', '.', '[SEP]', '[PAD]']\n",
      "['[CLS]', 'do', 'you', 'want', 'to', 'go', 'with', 'me', 'to', 'the', 'bank', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e903ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(len(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4afec9",
   "metadata": {},
   "source": [
    "How is the output stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "867235b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "134c59d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 13, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfcd7029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3800fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d5963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([13, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63cd355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can convert to numpy\n",
    "output[0][0].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15797d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2495878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6736561",
   "metadata": {},
   "source": [
    "Words with multiple meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e33cd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa665e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_river = output[0][0][6]\n",
    "bank_financial = output[0][1][6]\n",
    "bank_universal = output[0][2][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb76120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_matrix = np.concatenate((bank_river.numpy().reshape(1, 768), \n",
    "                              bank_financial.numpy().reshape(1, 768), \n",
    "                              bank_universal.numpy().reshape(1, 768)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f61a538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Universal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>River</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688735</td>\n",
       "      <td>0.770934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Investment</th>\n",
       "      <td>0.688735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universal</th>\n",
       "      <td>0.770934</td>\n",
       "      <td>0.836289</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               River  Investment  Universal\n",
       "River       1.000000    0.688735   0.770934\n",
       "Investment  0.688735    1.000000   0.836289\n",
       "Universal   0.770934    0.836289   1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(bank_matrix), \n",
    "             columns=['River', 'Investment', 'Universal'],\n",
    "             index=['River', 'Investment', 'Universal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584627f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba79ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'date', 'went', 'great', 'last', 'night', '!', '[SEP]', '[PAD]']\n",
      "['[CLS]', 'what', \"'\", 's', 'today', \"'\", 's', 'date', '?', '[SEP]']\n",
      "['[CLS]', 'this', 'date', 'is', 'too', 'sour', 'to', 'eat', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Calendar</th>\n",
       "      <th>Food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Relationship</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825439</td>\n",
       "      <td>0.743566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calendar</th>\n",
       "      <td>0.825439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.743566</td>\n",
       "      <td>0.771033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Relationship  Calendar      Food\n",
       "Relationship      1.000000  0.825439  0.743566\n",
       "Calendar          0.825439  1.000000  0.771033\n",
       "Food              0.743566  0.771033  1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"My date went great last night!\", \n",
    "        \"What's today's date?\", \n",
    "        \"This date is too sour to eat.\"]\n",
    "encoded_input = tokenizer(text, return_tensors='tf', padding = True)\n",
    "\n",
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) \n",
    "    \n",
    "output = model(**encoded_input)\n",
    "\n",
    "date_rel = output[0][0][2]\n",
    "date_time = output[0][1][7]\n",
    "date_food = output[0][2][2]\n",
    "\n",
    "date_matrix = np.concatenate((date_rel.numpy().reshape(1, 768), \n",
    "                              date_time.numpy().reshape(1, 768), \n",
    "                              date_food.numpy().reshape(1, 768)))\n",
    "\n",
    "pd.DataFrame(cosine_similarity(date_matrix), \n",
    "             columns=['Relationship', 'Calendar', 'Food'],\n",
    "             index=['Relationship', 'Calendar', 'Food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756d22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0fd5287",
   "metadata": {},
   "source": [
    "## 3. Sentiment classification without fine tuning --> embeddings + logit\n",
    "\n",
    "<img src='https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification-example.png' width=800>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47678127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# already loaded\n",
    "# import tensorflow as tf\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be7fbc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  a stirring , funny and finally transporting re...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo n't sit still f...      0\n",
       "3  this is a visually stunning rumination on love...      1\n",
       "4  jonathan parker 's bartleby should have been t...      1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                 delimiter='\\t', header=None)\n",
    "df.columns = ['review', 'label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c75850",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ee8daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "tokenized = df['review'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1b57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 67\n",
      "Median length: 22.0\n",
      "Mean length: 23.341907514450867\n"
     ]
    }
   ],
   "source": [
    "print('Max length:', tokenized.map(len).max())\n",
    "print('Median length:', tokenized.map(len).median())\n",
    "print('Mean length:', tokenized.map(len).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882e4d",
   "metadata": {},
   "source": [
    "Use tokenizer function that creates padded embeddings and outputs attention masks (what to consider, what not to consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61a77dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def bert_tokenizer(text):\n",
    "    \n",
    "    encoded_text = tokenizer.encode_plus(text,  max_length = MAX_LEN, truncation=True,  padding='max_length',  \n",
    "                                         return_attention_mask=True, return_tensors='tf')\n",
    "    \n",
    "    return encoded_text['input_ids'][0].numpy(), encoded_text['attention_mask'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a539c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  101,  7099,  3231,  2008,  2097,  2022, 20633,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer('Sample test that will be padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5414f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_padded, attention_masks = zip(*df['review'].apply(lambda x: bert_tokenizer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9421a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 30)\n",
      "(6920, 30)\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.convert_to_tensor(tokenized_padded)\n",
    "attention_mask = tf.convert_to_tensor(attention_masks)\n",
    "\n",
    "# input_ids = tf.stop_gradient(input_ids) # requires_grad=False in torch\n",
    "# attention_mask = tf.stop_gradient(attention_mask)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25c18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700b6199",
   "metadata": {},
   "source": [
    "#### 2. Apply BERT on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c7fe1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76afc852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([6920, 30, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5400989",
   "metadata": {},
   "source": [
    "6920 sentences, 30 words (tokens) in each sentence, 768 dimensions for each word (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351a115",
   "metadata": {},
   "source": [
    "#### 3. Get sentence embeddings out of the resuling tensor\n",
    "\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/6c2185c7620a3fe52f1968752febb6467723f4485c257442d3b0ed03bb0da197/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d6f75747075742d74656e736f722d73656c656374696f6e2e706e67' width=1000>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55e6f07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 768)\n",
      "(6920,)\n"
     ]
    }
   ],
   "source": [
    "X = last_hidden_states[0][:,0,:].numpy()\n",
    "y = df['label']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffac6d3",
   "metadata": {},
   "source": [
    "#### 4. Fit model, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a066f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4695acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 91, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4353dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(max_iter = 1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "790a5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = logit.predict(X_test)\n",
    "y_pred_prob = logit.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7dfc759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive class: 0.5216763005780347\n",
      "Accuracy: 0.8345375722543352\n",
      "AUC: 0.9162513712584235\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of positive class:', y.value_counts()[1] / df.shape[0])\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_class))\n",
    "print('AUC:', roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e7a140",
   "metadata": {},
   "source": [
    "#### 5. Predict sentiment of any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "276bd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \n",
    "    _tokenized, _attention_mask = bert_tokenizer(text)\n",
    "\n",
    "    _tokenized = _tokenized.reshape(1, 30)\n",
    "    _attention_mask = _attention_mask.reshape(1, 30)\n",
    "    _last_hidden_state = model(_tokenized, attention_mask = _attention_mask)\n",
    "    _X = _last_hidden_state[0][:,0,:][0].numpy().reshape(1, -1)\n",
    "\n",
    "    #predicted_class = logit.predict(_X)[0]\n",
    "    predicted_proba = logit.predict_proba(_X)[:, 1][0]\n",
    "\n",
    "    return print('Probability of being positive:', predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ed2dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.4586779717366192\n"
     ]
    }
   ],
   "source": [
    "text = 'I though the movie was going to suck, but actually it turned out to be really good.'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88518dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.2109336851428714\n"
     ]
    }
   ],
   "source": [
    "text = 'Overall OK, nothing special'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2abcc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.848766450385643\n"
     ]
    }
   ],
   "source": [
    "text = 'Liked it'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce9bd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6149149887169926\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2de94f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6892585731842714\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture!'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0043f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d998a2",
   "metadata": {},
   "source": [
    "## 4.  Sentiment classification with fine tuning --> BERT in neural net (`TFDistilBertForSequenceClassification`)\n",
    "\n",
    "<img src='https://skimai.com/wp-content/uploads/2020/03/Screen-Shot-2020-04-13-at-5.59.33-PM.png' width=800>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b04ba9",
   "metadata": {},
   "source": [
    "### 1. Create TF datasets\n",
    "- Tokenization\n",
    "- TF spedicif dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "748d8e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['label'], random_state = 91, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a5974ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def bert_tokenizer(text):    \n",
    "    encoded_text = tokenizer(text,  max_length = MAX_LEN, truncation=True,  padding='max_length', return_attention_mask=True, return_tensors='tf')    \n",
    "    return encoded_text\n",
    "\n",
    "def construct_tfdataset(encoded_text, y = None):    \n",
    "    if y:\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encoded_text),y))\n",
    "    else: # for testing / new samples\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "17fcda34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(bert_tokenizer('hello there')).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "487bfed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_encodings = bert_tokenizer(X_train.tolist())\n",
    "X_test_encodings = bert_tokenizer(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2deabb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH_SIZE = 500\n",
    "train = construct_tfdataset(X_train_encodings, y_train.tolist()).batch(BATCH_SIZE)\n",
    "test = construct_tfdataset(X_test_encodings, y_test.tolist()).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512831b",
   "metadata": {},
   "source": [
    "### 2. Import and compile BERT model for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4bbb6421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "785a2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "31e73387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14206c",
   "metadata": {},
   "source": [
    "### 3. Fine tune model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c3e6a",
   "metadata": {},
   "source": [
    "Don't want to train the BERT layer due to resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e752f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3f8ef46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 592,130\n",
      "Non-trainable params: 66,362,880\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e4b60b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "12/12 [==============================] - 1145s 94s/step - loss: 0.6482 - accuracy: 0.6404\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 1150s 96s/step - loss: 0.4322 - accuracy: 0.8293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ef88b44a00>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "model.fit(train, batch_size = BATCH_SIZE, epochs = N_EPOCHS, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79e46b",
   "metadata": {},
   "source": [
    "### 4. Evaluate and make new predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ffa1008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 60s 19s/step - loss: 0.3639 - accuracy: 0.8432\n",
      "{'loss': 0.3639087677001953, 'accuracy': 0.8432080745697021}\n"
     ]
    }
   ],
   "source": [
    "test_eval = model.evaluate(test, return_dict = True, batch_size = BATCH_SIZE)\n",
    "print(test_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f1094a",
   "metadata": {},
   "source": [
    "After **2 epochs** while **BERT layer was frozen**, test accuracy **beat logit by 2%points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d0531183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_prediction(text):\n",
    "    \n",
    "    x = [text]\n",
    "\n",
    "    encodings = bert_tokenizer(x)\n",
    "    tfdataset = construct_tfdataset(encodings)\n",
    "    tfdataset = tfdataset.batch(1)\n",
    "\n",
    "    preds = model.predict(tfdataset).logits\n",
    "    preds = tf.keras.activations.softmax(tf.convert_to_tensor(preds)).numpy()\n",
    "    \n",
    "    return preds[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "480acee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1069365"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_prediction('This movie sucked ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9e4cd434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545717"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_prediction('This movie defines the golden standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dd76e94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8001689"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_prediction('I have never seen such a terrific picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9900a5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18013132"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_prediction('I have never seen such a terrible picture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "aedf0537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16001002"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_prediction('Wouldnt really watch it again')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6053de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402acbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4746e569",
   "metadata": {},
   "source": [
    "## 5. Sentiment classification with fine tuning --> BERT in neural net (`TFDistilBertModel`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c74fa",
   "metadata": {},
   "source": [
    "`TFDistilBertForSequenceClassification` already has a classification structure. With `TFDistilBertModel` we just get the BERT layer (transformer layer) for which we need to define:\n",
    "\n",
    "1. input\n",
    "    - token IDs\n",
    "    - attention masks\n",
    "2. classifier head (or multiple layers) and it's activation depending on the # of classes\n",
    "3. any additional layers (dropouts, dense connections, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72191f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0613764",
   "metadata": {},
   "source": [
    "## BERT Tutorial\n",
    "### How to leverage BERT models for NLP use cases\n",
    "\n",
    "<img src='https://towardsml.files.wordpress.com/2019/09/bert.png?w=1400' width=450>\n",
    "\n",
    "#### A lot of available models - choose according to computational powers\n",
    "\n",
    "Link: https://github.com/google-research/bert/\n",
    "\n",
    "<img src='data/bert_models.png' width=600>\n",
    "\n",
    "\n",
    "#### I will use DistilBert - smaller BERT that reaches similarly good performance level\n",
    "\n",
    "DistilBert\n",
    "- 66m parameters (Bert 110m)\n",
    "- Layers / Hidden dimensions / Attention heads: 6 / 768 / 12 (BERT: 12 / 768 / 12)\n",
    "- Performance: 97% of BERT\n",
    "\n",
    "Complete documentation: https://huggingface.co/docs/transformers/model_doc/distilbert#distilbert (actually very user friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8e03c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version: 1.10.2\n",
      "Transformers version: 4.17.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "print('Python version:', sys.version)\n",
    "print('PyTorch version:', torch.__version__)\n",
    "print('Transformers version:', transformers_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1f89bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b2d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# torch.set_num_threads(4)\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6d996",
   "metadata": {},
   "source": [
    "- DistilBertTokenizer: tokenizes input sequence\n",
    "- DistilBertModel: creates embeddings on top of tokenized sequence (DistilBertTokenizer + training embeddings)\n",
    "    - if using TensorFlow: TFDistilBertModel\n",
    "- DistilBertForSequenceClassification: already builds a classifier on top of embeddings (DistilBertModel + classifier)\n",
    "    - if using TensorFlow: TFDistilBertForSequenceClassification\n",
    "- pipeline: DIY use cases    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e2824",
   "metadata": {},
   "source": [
    "## 1. Try out [MASK] performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a194e091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [00:42<00:00, 6.24MB/s] \n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "192d9c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10261113196611404,\n",
       "  'token': 3637,\n",
       "  'token_str': 'sleep',\n",
       "  'sequence': 'she wanted to go to sleep.'},\n",
       " {'score': 0.06890519708395004,\n",
       "  'token': 6014,\n",
       "  'token_str': 'heaven',\n",
       "  'sequence': 'she wanted to go to heaven.'},\n",
       " {'score': 0.055472031235694885,\n",
       "  'token': 2793,\n",
       "  'token_str': 'bed',\n",
       "  'sequence': 'she wanted to go to bed.'},\n",
       " {'score': 0.029796097427606583,\n",
       "  'token': 7173,\n",
       "  'token_str': 'jail',\n",
       "  'sequence': 'she wanted to go to jail.'},\n",
       " {'score': 0.02460319548845291,\n",
       "  'token': 2267,\n",
       "  'token_str': 'college',\n",
       "  'sequence': 'she wanted to go to college.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('She wanted to go to [MASK].', top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a88714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.04563814774155617,\n",
       "  'token': 21714,\n",
       "  'token_str': 'bearings',\n",
       "  'sequence': \"i can't find my bearings.\"},\n",
       " {'score': 0.029267525300383568,\n",
       "  'token': 3042,\n",
       "  'token_str': 'phone',\n",
       "  'sequence': \"i can't find my phone.\"},\n",
       " {'score': 0.0241349246352911,\n",
       "  'token': 3437,\n",
       "  'token_str': 'answer',\n",
       "  'sequence': \"i can't find my answer.\"},\n",
       " {'score': 0.023151548579335213,\n",
       "  'token': 6998,\n",
       "  'token_str': 'answers',\n",
       "  'sequence': \"i can't find my answers.\"},\n",
       " {'score': 0.02219867706298828,\n",
       "  'token': 3611,\n",
       "  'token_str': 'dad',\n",
       "  'sequence': \"i can't find my dad.\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I can't find my [MASK] .\", top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f315e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08036588877439499,\n",
       "  'token': 6898,\n",
       "  'token_str': 'boyfriend',\n",
       "  'sequence': 'i wish i had a boyfriend.'},\n",
       " {'score': 0.04185354337096214,\n",
       "  'token': 3336,\n",
       "  'token_str': 'baby',\n",
       "  'sequence': 'i wish i had a baby.'},\n",
       " {'score': 0.030962644144892693,\n",
       "  'token': 6513,\n",
       "  'token_str': 'girlfriend',\n",
       "  'sequence': 'i wish i had a girlfriend.'},\n",
       " {'score': 0.024552473798394203,\n",
       "  'token': 3382,\n",
       "  'token_str': 'chance',\n",
       "  'sequence': 'i wish i had a chance.'},\n",
       " {'score': 0.020696187391877174,\n",
       "  'token': 3959,\n",
       "  'token_str': 'dream',\n",
       "  'sequence': 'i wish i had a dream.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I wish I had a [MASK].\", top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b376cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13283944129943848,\n",
       "  'token': 13877,\n",
       "  'token_str': 'waitress',\n",
       "  'sequence': 'the black woman worked as a waitress.'},\n",
       " {'score': 0.12586164474487305,\n",
       "  'token': 6821,\n",
       "  'token_str': 'nurse',\n",
       "  'sequence': 'the black woman worked as a nurse.'},\n",
       " {'score': 0.11708816140890121,\n",
       "  'token': 10850,\n",
       "  'token_str': 'maid',\n",
       "  'sequence': 'the black woman worked as a maid.'},\n",
       " {'score': 0.11500067263841629,\n",
       "  'token': 19215,\n",
       "  'token_str': 'prostitute',\n",
       "  'sequence': 'the black woman worked as a prostitute.'},\n",
       " {'score': 0.0472276546061039,\n",
       "  'token': 22583,\n",
       "  'token_str': 'housekeeper',\n",
       "  'sequence': 'the black woman worked as a housekeeper.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The black woman worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986c686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.12353681027889252,\n",
       "  'token': 20987,\n",
       "  'token_str': 'blacksmith',\n",
       "  'sequence': 'the white man worked as a blacksmith.'},\n",
       " {'score': 0.10142574459314346,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the white man worked as a carpenter.'},\n",
       " {'score': 0.049850210547447205,\n",
       "  'token': 7500,\n",
       "  'token_str': 'farmer',\n",
       "  'sequence': 'the white man worked as a farmer.'},\n",
       " {'score': 0.03932555019855499,\n",
       "  'token': 18594,\n",
       "  'token_str': 'miner',\n",
       "  'sequence': 'the white man worked as a miner.'},\n",
       " {'score': 0.033517707139253616,\n",
       "  'token': 14998,\n",
       "  'token_str': 'butcher',\n",
       "  'sequence': 'the white man worked as a butcher.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The white man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30cab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.056957632303237915,\n",
       "  'token': 12421,\n",
       "  'token_str': 'excluded',\n",
       "  'sequence': 'black people are excluded.'},\n",
       " {'score': 0.032912153750658035,\n",
       "  'token': 22216,\n",
       "  'token_str': 'enslaved',\n",
       "  'sequence': 'black people are enslaved.'},\n",
       " {'score': 0.0325375571846962,\n",
       "  'token': 8135,\n",
       "  'token_str': 'christians',\n",
       "  'sequence': 'black people are christians.'},\n",
       " {'score': 0.02683640830218792,\n",
       "  'token': 14302,\n",
       "  'token_str': 'minorities',\n",
       "  'sequence': 'black people are minorities.'},\n",
       " {'score': 0.017561351880431175,\n",
       "  'token': 27666,\n",
       "  'token_str': 'persecuted',\n",
       "  'sequence': 'black people are persecuted.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Black people are [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5370825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26134654",
   "metadata": {},
   "source": [
    "## 2. Get features (embeddings) of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2c9ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9f92",
   "metadata": {},
   "source": [
    "return_tensors (`str` or [`~file_utils.TensorType`], *optional*):\n",
    "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "\n",
    "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
    "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
    "        - `'np'`: Return Numpy `np.ndarray` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b2e941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01607d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8671ffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3a05d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab08db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I went to the river bank and just laid there.\", \n",
    "        \"I work at an investment bank in New York.\", \n",
    "        \"Do you want to go with me to the bank?\"]\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01142949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1998, 2074, 4201, 2045, 1012,\n",
       "          102],\n",
       "        [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1999, 2047, 2259, 1012,  102,\n",
       "            0],\n",
       "        [ 101, 2079, 2017, 2215, 2000, 2175, 2007, 2033, 2000, 1996, 2924, 1029,\n",
       "          102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4742c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1998, 2074, 4201, 2045, 1012,\n",
       "          102],\n",
       "        [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1999, 2047, 2259, 1012,  102,\n",
       "            0],\n",
       "        [ 101, 2079, 2017, 2215, 2000, 2175, 2007, 2033, 2000, 1996, 2924, 1029,\n",
       "          102]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e32cc983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1998, 2074, 4201, 2045,\n",
       "        1012,  102],\n",
       "       [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1999, 2047, 2259, 1012,\n",
       "         102,    0],\n",
       "       [ 101, 2079, 2017, 2215, 2000, 2175, 2007, 2033, 2000, 1996, 2924,\n",
       "        1029,  102]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "006e57ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29bf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0d2be74",
   "metadata": {},
   "source": [
    "How does the tokenized text look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31155965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'went', 'to', 'the', 'river', 'bank', 'and', 'just', 'laid', 'there', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'work', 'at', 'an', 'investment', 'bank', 'in', 'new', 'york', '.', '[SEP]', '[PAD]']\n",
      "['[CLS]', 'do', 'you', 'want', 'to', 'go', 'with', 'me', 'to', 'the', 'bank', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e903ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(len(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4afec9",
   "metadata": {},
   "source": [
    "How is the output stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "867235b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "134c59d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 13, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfcd7029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3800fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66d5963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63cd355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 768)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can convert to numpy\n",
    "output[0][0].detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15797d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2495878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6736561",
   "metadata": {},
   "source": [
    "Words with multiple meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e33cd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa665e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_river = output[0][0][6]\n",
    "bank_financial = output[0][1][6]\n",
    "bank_universal = output[0][2][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb76120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_matrix = np.concatenate((bank_river.detach().numpy().reshape(1, 768), \n",
    "                              bank_financial.detach().numpy().reshape(1, 768), \n",
    "                              bank_universal.detach().numpy().reshape(1, 768)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f61a538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Universal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>River</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688735</td>\n",
       "      <td>0.770934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Investment</th>\n",
       "      <td>0.688735</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universal</th>\n",
       "      <td>0.770934</td>\n",
       "      <td>0.836289</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               River  Investment  Universal\n",
       "River       1.000000    0.688735   0.770934\n",
       "Investment  0.688735    1.000000   0.836289\n",
       "Universal   0.770934    0.836289   1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(bank_matrix), \n",
    "             columns=['River', 'Investment', 'Universal'],\n",
    "             index=['River', 'Investment', 'Universal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584627f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba79ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'date', 'went', 'great', 'last', 'night', '!', '[SEP]', '[PAD]']\n",
      "['[CLS]', 'what', \"'\", 's', 'today', \"'\", 's', 'date', '?', '[SEP]']\n",
      "['[CLS]', 'this', 'date', 'is', 'too', 'sour', 'to', 'eat', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Calendar</th>\n",
       "      <th>Food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Relationship</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825439</td>\n",
       "      <td>0.743566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calendar</th>\n",
       "      <td>0.825439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.743566</td>\n",
       "      <td>0.771033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Relationship  Calendar      Food\n",
       "Relationship      1.000000  0.825439  0.743566\n",
       "Calendar          0.825439  1.000000  0.771033\n",
       "Food              0.743566  0.771033  1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"My date went great last night!\", \n",
    "        \"What's today's date?\", \n",
    "        \"This date is too sour to eat.\"]\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding = True)\n",
    "\n",
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) \n",
    "    \n",
    "output = model(**encoded_input)\n",
    "\n",
    "date_rel = output[0][0][2]\n",
    "date_time = output[0][1][7]\n",
    "date_food = output[0][2][2]\n",
    "\n",
    "date_matrix = np.concatenate((date_rel.detach().numpy().reshape(1, 768), \n",
    "                              date_time.detach().numpy().reshape(1, 768), \n",
    "                              date_food.detach().numpy().reshape(1, 768)))\n",
    "\n",
    "pd.DataFrame(cosine_similarity(date_matrix), \n",
    "             columns=['Relationship', 'Calendar', 'Food'],\n",
    "             index=['Relationship', 'Calendar', 'Food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756d22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0fd5287",
   "metadata": {},
   "source": [
    "## 3. Transfer learning without fine tuning - sentiment classification\n",
    "\n",
    "<img src='https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification-example.png' width=800>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47678127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# already loaded\n",
    "# import torch\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7fbc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  a stirring , funny and finally transporting re...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo n't sit still f...      0\n",
       "3  this is a visually stunning rumination on love...      1\n",
       "4  jonathan parker 's bartleby should have been t...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                 delimiter='\\t', header=None)\n",
    "df.columns = ['review', 'label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c75850",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ee8daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "tokenized = df['review'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be1b57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 67\n",
      "Median length: 22.0\n",
      "Mean length: 23.341907514450867\n"
     ]
    }
   ],
   "source": [
    "print('Max length:', tokenized.map(len).max())\n",
    "print('Median length:', tokenized.map(len).median())\n",
    "print('Mean length:', tokenized.map(len).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882e4d",
   "metadata": {},
   "source": [
    "Use tokenizer function that creates padded embeddings and outputs attention masks (what to consider, what not to consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61a77dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def bert_tokenizer(text):\n",
    "    \n",
    "    encoded_text = tokenizer.encode_plus(text,  max_length = MAX_LEN, truncation=True,  padding='max_length',  \n",
    "                                         return_attention_mask=True, return_tensors='tf')\n",
    "    \n",
    "    return encoded_text['input_ids'][0].numpy(), encoded_text['attention_mask'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a539c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  101,  7099,  3231,  2008,  2097,  2022, 20633,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0]),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer('Sample test that will be padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5414f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_padded, attention_masks = zip(*df['review'].apply(lambda x: bert_tokenizer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca289e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6920, 30])\n",
      "torch.Size([6920, 30])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(np.array(tokenized_padded))  # could add requires_grad=False in torch, torch.no_grad() is more universal\n",
    "attention_mask = torch.tensor(np.array(attention_masks))\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b6199",
   "metadata": {},
   "source": [
    "#### 2. Apply BERT on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40c37649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with torch.no_grad(): # no need to keep track of gradients\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76afc852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6920, 30, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5400989",
   "metadata": {},
   "source": [
    "6920 sentences, 30 words (tokens) in each sentence, 768 dimensions for each word (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351a115",
   "metadata": {},
   "source": [
    "#### 3. Get sentence embeddings out of the resuling tensor\n",
    "\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/6c2185c7620a3fe52f1968752febb6467723f4485c257442d3b0ed03bb0da197/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d6f75747075742d74656e736f722d73656c656374696f6e2e706e67' width=1000>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55e6f07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 768)\n",
      "(6920,)\n"
     ]
    }
   ],
   "source": [
    "X = last_hidden_states[0][:,0,:].numpy()\n",
    "y = df['label']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffac6d3",
   "metadata": {},
   "source": [
    "#### 4. Fit model, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a066f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4695acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 91, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4353dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(max_iter = 1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "790a5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = logit.predict(X_test)\n",
    "y_pred_prob = logit.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7dfc759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive class: 0.5216763005780347\n",
      "Accuracy: 0.8345375722543352\n",
      "AUC: 0.9162325654286162\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of positive class:', y.value_counts()[1] / df.shape[0])\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_class))\n",
    "print('AUC:', roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1060fd95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e7a140",
   "metadata": {},
   "source": [
    "#### 5. Predict sentiment of any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c35f9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \n",
    "    _tokenized, _attention_mask = bert_tokenizer(text)\n",
    "\n",
    "    _tokenized = torch.reshape(torch.from_numpy(_tokenized), (1, 30))\n",
    "    _attention_mask = torch.reshape(torch.from_numpy(_attention_mask), (1, 30))\n",
    "    _last_hidden_state = model(_tokenized, attention_mask = _attention_mask)\n",
    "    _X = _last_hidden_state[0][:,0,:][0].detach().numpy().reshape(1, -1)\n",
    "\n",
    "    #predicted_class = logit.predict(_X)[0]\n",
    "    predicted_proba = logit.predict_proba(_X)[:, 1][0]\n",
    "\n",
    "    return print('Probability of being positive:', predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ed2dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.4585272365085751\n"
     ]
    }
   ],
   "source": [
    "text = 'I though the movie was going to suck, but actually it turned out to be really good.'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88518dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.21126795797738587\n"
     ]
    }
   ],
   "source": [
    "text = 'Overall OK, nothing special'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2abcc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.8487172901582756\n"
     ]
    }
   ],
   "source": [
    "text = 'Liked it'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce9bd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6141236449664051\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2de94f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6883185167869081\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture!'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0043f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d998a2",
   "metadata": {},
   "source": [
    "## 4.  Sentiment classification with fine tuning --> BERT in neural net (`DistilBertForSequenceClassification`)\n",
    "\n",
    "<img src='https://skimai.com/wp-content/uploads/2020/03/Screen-Shot-2020-04-13-at-5.59.33-PM.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d240b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# could use sampler (random for train, sequential for test)\n",
    "\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20b852f",
   "metadata": {},
   "source": [
    "### 1. Create PyTorch datasets\n",
    "- Tokenization\n",
    "- TF spedicif dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c4e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, random_state = 91, train_size = 0.8)\n",
    "\n",
    "MAX_LEN = 30\n",
    "BATCH_SIZE = 300\n",
    "EPOCHS = 2\n",
    "\n",
    "def bert_tokenizer(text):    \n",
    "    encoded_text = tokenizer(text,  max_length = MAX_LEN, truncation=True,  padding='max_length', return_attention_mask=True, return_tensors='pt')    \n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec046fd",
   "metadata": {},
   "source": [
    "Convert to PyTorch tensors --> Datasets --> Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efca1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_tokenized = bert_tokenizer(train['review'].tolist())\n",
    "test_tokenized = bert_tokenizer(test['review'].tolist())\n",
    "\n",
    "train_input_ids = train_tokenized['input_ids']\n",
    "test_input_ids = test_tokenized['input_ids']\n",
    "\n",
    "train_attention_mask = train_tokenized['attention_mask']\n",
    "test_attention_mask = test_tokenized['attention_mask']\n",
    "\n",
    "train_labels = torch.Tensor(train['label'].values).type(torch.LongTensor)\n",
    "test_labels = torch.Tensor(test['label'].values).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c85d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393ca73",
   "metadata": {},
   "source": [
    "### 2. Import and compile BERT model for sequence classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a341413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \n",
    "                                                            num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "16f54016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.6843, grad_fn=<NllLossBackward0>), logits=tensor([[-0.0695,  0.0219],\n",
       "        [-0.2379, -0.1047]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.tensor([101, 102]).reshape(-1,1), labels = torch.tensor([0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f25b9247",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 3e-5)\n",
    "#loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617e3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da1c200f",
   "metadata": {},
   "source": [
    "### 3. Fine tune model\n",
    "\n",
    "Optionally freeze BERT layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "49b96d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5791001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find layers and set their grads to False\n",
    "# params = list(model.parameters())\n",
    "\n",
    "# for param in params[:-4]:\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8a90f792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "  Batch     0  of     19.    Elapsed: 0:00:00.\n",
      "  Batch     4  of     19.    Elapsed: 0:01:59.\n",
      "  Batch     8  of     19.    Elapsed: 0:03:45.\n",
      "  Batch    12  of     19.    Elapsed: 0:05:44.\n",
      "  Batch    16  of     19.    Elapsed: 0:07:39.\n",
      "\n",
      "Train -- loss: 0.577736 -- accuracy: 0.704391\n",
      "Validation -- loss: 0.401456 -- accuracy: 0.826304\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "  Batch     0  of     19.    Elapsed: 0:00:00.\n",
      "  Batch     4  of     19.    Elapsed: 0:01:57.\n",
      "  Batch     8  of     19.    Elapsed: 0:03:46.\n",
      "  Batch    12  of     19.    Elapsed: 0:05:41.\n",
      "  Batch    16  of     19.    Elapsed: 0:07:41.\n",
      "\n",
      "Train -- loss: 0.316938 -- accuracy: 0.872332\n",
      "Validation -- loss: 0.333243 -- accuracy: 0.864797\n",
      "\n",
      "\n",
      "Done!\n",
      "Wall time: 19min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    \n",
    "    ### TRAIN ### \n",
    "    model.train()\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_steps = 0 \n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # print batch progress\n",
    "        if step % 4 == 0 : #and not step == 0\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        seq_output = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)        \n",
    "        loss =  seq_output.loss\n",
    "        pred = seq_output.logits\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())  \n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        pred = pred.detach().numpy()\n",
    "        labels = b_labels.numpy()        \n",
    "        tmp_tr_accuracy = flat_accuracy(pred, labels)\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "        \n",
    "\n",
    "    print(\"\\nTrain -- loss: {:>8f} -- accuracy: {:>8f}\".format(tr_loss / nb_tr_steps, tr_accuracy / nb_tr_steps))\n",
    "\n",
    "    ### EVALUATE ###\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            \n",
    "            seq_output = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss =  seq_output.loss\n",
    "            pred = seq_output.logits\n",
    "        \n",
    "        test_loss.append(loss.item())  \n",
    "        eval_loss += loss.item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        pred = pred.detach().numpy()\n",
    "        labels = b_labels.numpy()        \n",
    "        tmp_eval_accuracy = flat_accuracy(pred, labels)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    print(\"Validation -- loss: {:>8f} -- accuracy: {:>8f}\\n\\n\".format(eval_loss / nb_eval_steps, eval_accuracy / nb_eval_steps))\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff65733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28027b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03508779",
   "metadata": {},
   "source": [
    "### 4. Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2426ac35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "torch_predictions = model(test_dataloader.dataset.tensors[0], \n",
    "                          attention_mask = test_dataloader.dataset.tensors[1],\n",
    "                          labels = test_dataloader.dataset.tensors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3b3f8a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits'])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6ee30dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.3283, grad_fn=<NllLossBackward0>), logits=tensor([[-1.9799,  1.4200],\n",
       "        [-2.3359,  1.6462],\n",
       "        [-0.3222, -0.0718],\n",
       "        ...,\n",
       "        [ 0.3872, -0.6693],\n",
       "        [-1.2620,  0.7753],\n",
       "        [-0.4462,  0.1451]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f62b2cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive class: 0.5122832369942196\n",
      "Accuracy: 0.8684971098265896\n",
      "AUC: 0.9359431646032493\n"
     ]
    }
   ],
   "source": [
    "y_tst = test_dataloader.dataset.tensors[2].numpy()\n",
    "logits_tst = torch_predictions.logits\n",
    "probs = F.softmax(logits_tst, 1).detach().numpy()[:,1]\n",
    "\n",
    "print('Ratio of positive class:', np.unique(y_tst, return_counts=True)[1][1] / len(y_tst))\n",
    "print('Accuracy:', flat_accuracy(logits_tst.detach().numpy(), y_tst))\n",
    "print('AUC:', roc_auc_score(y_tst, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f951ab5",
   "metadata": {},
   "source": [
    "## Great improvement in just 2 epochs\n",
    "\n",
    "Compared to embeddings + logit, the embeddings + classifier layer in neural net:\n",
    "\n",
    "### Accuracy: 86.8% from 83.4%\n",
    "### AUC: 93.5 from 91.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe9fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5317fff6",
   "metadata": {},
   "source": [
    "### 5. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a9f6cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment_net(text):\n",
    "    \n",
    "    tokenized_text = bert_tokenizer(text)\n",
    "    logits = model(**tokenized_text).logits\n",
    "    prob = F.softmax(logits, 1).detach().numpy()[0][1]\n",
    "\n",
    "    return print('Probability of being positive:', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bb2f3d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.96273935\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment_net('nice movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "be7043b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.054781504\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment_net('movie sucked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dda9bf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.8789064\n"
     ]
    }
   ],
   "source": [
    "text = 'I though the movie was going to suck, but actually it turned out to be really good.'\n",
    "predict_sentiment_net(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b03304f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.07032349\n"
     ]
    }
   ],
   "source": [
    "text = 'Overall OK, nothing special'\n",
    "predict_sentiment_net(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15d7a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.94001234\n"
     ]
    }
   ],
   "source": [
    "text = 'Liked it'\n",
    "predict_sentiment_net(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "700cfc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6782087\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture'\n",
    "predict_sentiment_net(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3f1dd06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.7235411\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture!'\n",
    "predict_sentiment_net(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c7839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

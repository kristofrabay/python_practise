{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0613764",
   "metadata": {},
   "source": [
    "## BERT Tutorial\n",
    "### How to leverage BERT models for NLP use cases\n",
    "\n",
    "<img src='https://towardsml.files.wordpress.com/2019/09/bert.png?w=1400' width=450>\n",
    "\n",
    "#### A lot of available models - choose according to computational powers\n",
    "\n",
    "Link: https://github.com/google-research/bert/\n",
    "\n",
    "<img src='data/bert_models.png' width=600>\n",
    "\n",
    "\n",
    "#### I will use DistilBert - smaller BERT that reaches similarly good performance level\n",
    "\n",
    "DistilBert\n",
    "- 66m parameters (Bert 110m)\n",
    "- Layers / Hidden dimensions / Attention heads: 6 / 768 / 12 (BERT: 12 / 768 / 12)\n",
    "- Performance: 97% of BERT\n",
    "\n",
    "Complete documentation: https://huggingface.co/docs/transformers/model_doc/distilbert#distilbert (actually very user friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06b2d70b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ntype object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2776\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0maudio_classification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAudioClassificationPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mautomatic_speech_recognition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutomaticSpeechRecognitionPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\audio_classification.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPIPELINE_INIT_ARGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_end_docstrings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodelcard\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfiguration_auto\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modelcard.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m )\n\u001b[1;32m---> 58\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\training_args.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m )\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvaluationStrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHubStrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIntervalStrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSchedulerType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShardedDDPOption\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;31m# from tensorflow.python import keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;31m# from tensorflow.python.layers import layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=unused-import,line-too-long,wildcard-import,g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_tf_layers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\models.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\activations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Image preprocessing layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_preprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCenterCrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_preprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomCrop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\preprocessing\\image_preprocessing.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimage_preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontrol_flow_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimeseries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mall_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\all_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_source_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_gpu_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmulti_gpu_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\multi_gpu_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\h5py\\version.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mh5\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_h5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5.pyx\u001b[0m in \u001b[0;36minit h5py.h5\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e021ab8d9e93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# for pytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDistilBertForSequenceClassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# for tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2765\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2766\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2767\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2768\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2769\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2777\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2778\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2779\u001b[1;33m             raise RuntimeError(\n\u001b[0m\u001b[0;32m   2780\u001b[0m                 \u001b[1;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its traceback):\\n{e}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2781\u001b[0m             ) from e\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\ntype object 'h5py.h5.H5PYConfig' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "\n",
    "# for pytorch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification, pipeline\n",
    "\n",
    "# for tensorflow\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel, TFDistilBertForSequenceClassification, pipeline\n",
    "\n",
    "# tensorflow helpers\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96f91f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.19.2\n",
      "  Downloading numpy-1.19.2-cp38-cp38-win_amd64.whl (13.0 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.2\n",
      "    Uninstalling numpy-1.21.2:\n",
      "      Successfully uninstalled numpy-1.21.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] A hozzáférés megtagadva: 'C:\\\\Users\\\\krist\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-p0ob9ox4\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy==1.19.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504d6455",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a904e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5a6d996",
   "metadata": {},
   "source": [
    "- DistilBertTokenizer: tokenizes input sequence\n",
    "- DistilBertModel: creates embeddings on top of tokenized sequence (DistilBertTokenizer + training embeddings)\n",
    "    - if using TensorFlow: TFDistilBertModel\n",
    "- DistilBertForSequenceClassification: already builds a classifier on top of embeddings (DistilBertModel + classifier)\n",
    "    - if using TensorFlow: TFDistilBertForSequenceClassification\n",
    "- pipeline: DIY use cases    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e2824",
   "metadata": {},
   "source": [
    "## 1. Try out [MASK] performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a194e091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ba91f6fd9b4fbeb96092937f3b02fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032b620ed8284f3e9531c752422c1aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8303b6c5ef9f4635ae5c86276ef99f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123523edd4414094971f63ae0cdd9812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45fe5eb22234fc79a4437fdc61256ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192d9c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.10261130332946777,\n",
       "  'token': 3637,\n",
       "  'token_str': 'sleep',\n",
       "  'sequence': 'she wanted to go to sleep.'},\n",
       " {'score': 0.06890508532524109,\n",
       "  'token': 6014,\n",
       "  'token_str': 'heaven',\n",
       "  'sequence': 'she wanted to go to heaven.'},\n",
       " {'score': 0.05547202005982399,\n",
       "  'token': 2793,\n",
       "  'token_str': 'bed',\n",
       "  'sequence': 'she wanted to go to bed.'},\n",
       " {'score': 0.02979607693850994,\n",
       "  'token': 7173,\n",
       "  'token_str': 'jail',\n",
       "  'sequence': 'she wanted to go to jail.'},\n",
       " {'score': 0.024603189900517464,\n",
       "  'token': 2267,\n",
       "  'token_str': 'college',\n",
       "  'sequence': 'she wanted to go to college.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('She wanted to go to [MASK].', top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a88714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.04563814401626587,\n",
       "  'token': 21714,\n",
       "  'token_str': 'bearings',\n",
       "  'sequence': \"i can't find my bearings.\"},\n",
       " {'score': 0.029267553240060806,\n",
       "  'token': 3042,\n",
       "  'token_str': 'phone',\n",
       "  'sequence': \"i can't find my phone.\"},\n",
       " {'score': 0.024135015904903412,\n",
       "  'token': 3437,\n",
       "  'token_str': 'answer',\n",
       "  'sequence': \"i can't find my answer.\"},\n",
       " {'score': 0.023151546716690063,\n",
       "  'token': 6998,\n",
       "  'token_str': 'answers',\n",
       "  'sequence': \"i can't find my answers.\"},\n",
       " {'score': 0.02219865471124649,\n",
       "  'token': 3611,\n",
       "  'token_str': 'dad',\n",
       "  'sequence': \"i can't find my dad.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I can't find my [MASK] .\", top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f315e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.0803661048412323,\n",
       "  'token': 6898,\n",
       "  'token_str': 'boyfriend',\n",
       "  'sequence': 'i wish i had a boyfriend.'},\n",
       " {'score': 0.04185345396399498,\n",
       "  'token': 3336,\n",
       "  'token_str': 'baby',\n",
       "  'sequence': 'i wish i had a baby.'},\n",
       " {'score': 0.030962727963924408,\n",
       "  'token': 6513,\n",
       "  'token_str': 'girlfriend',\n",
       "  'sequence': 'i wish i had a girlfriend.'},\n",
       " {'score': 0.024552376940846443,\n",
       "  'token': 3382,\n",
       "  'token_str': 'chance',\n",
       "  'sequence': 'i wish i had a chance.'},\n",
       " {'score': 0.020696144551038742,\n",
       "  'token': 3959,\n",
       "  'token_str': 'dream',\n",
       "  'sequence': 'i wish i had a dream.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"I wish I had a [MASK].\", top_k = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b376cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.13283953070640564,\n",
       "  'token': 13877,\n",
       "  'token_str': 'waitress',\n",
       "  'sequence': 'the black woman worked as a waitress.'},\n",
       " {'score': 0.1258619725704193,\n",
       "  'token': 6821,\n",
       "  'token_str': 'nurse',\n",
       "  'sequence': 'the black woman worked as a nurse.'},\n",
       " {'score': 0.11708834767341614,\n",
       "  'token': 10850,\n",
       "  'token_str': 'maid',\n",
       "  'sequence': 'the black woman worked as a maid.'},\n",
       " {'score': 0.11499997973442078,\n",
       "  'token': 19215,\n",
       "  'token_str': 'prostitute',\n",
       "  'sequence': 'the black woman worked as a prostitute.'},\n",
       " {'score': 0.047227732837200165,\n",
       "  'token': 22583,\n",
       "  'token_str': 'housekeeper',\n",
       "  'sequence': 'the black woman worked as a housekeeper.'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The black woman worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "986c686e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.12353688478469849,\n",
       "  'token': 20987,\n",
       "  'token_str': 'blacksmith',\n",
       "  'sequence': 'the white man worked as a blacksmith.'},\n",
       " {'score': 0.10142599791288376,\n",
       "  'token': 10533,\n",
       "  'token_str': 'carpenter',\n",
       "  'sequence': 'the white man worked as a carpenter.'},\n",
       " {'score': 0.04984995722770691,\n",
       "  'token': 7500,\n",
       "  'token_str': 'farmer',\n",
       "  'sequence': 'the white man worked as a farmer.'},\n",
       " {'score': 0.0393255352973938,\n",
       "  'token': 18594,\n",
       "  'token_str': 'miner',\n",
       "  'sequence': 'the white man worked as a miner.'},\n",
       " {'score': 0.03351772576570511,\n",
       "  'token': 14998,\n",
       "  'token_str': 'butcher',\n",
       "  'sequence': 'the white man worked as a butcher.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"The white man worked as a [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f30cab53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.056957732886075974,\n",
       "  'token': 12421,\n",
       "  'token_str': 'excluded',\n",
       "  'sequence': 'black people are excluded.'},\n",
       " {'score': 0.0329122394323349,\n",
       "  'token': 22216,\n",
       "  'token_str': 'enslaved',\n",
       "  'sequence': 'black people are enslaved.'},\n",
       " {'score': 0.0325375497341156,\n",
       "  'token': 8135,\n",
       "  'token_str': 'christians',\n",
       "  'sequence': 'black people are christians.'},\n",
       " {'score': 0.02683648094534874,\n",
       "  'token': 14302,\n",
       "  'token_str': 'minorities',\n",
       "  'sequence': 'black people are minorities.'},\n",
       " {'score': 0.017561450600624084,\n",
       "  'token': 27666,\n",
       "  'token_str': 'persecuted',\n",
       "  'sequence': 'black people are persecuted.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Black people are [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5370825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26134654",
   "metadata": {},
   "source": [
    "## 2. Get features (embeddings) of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9b9f92",
   "metadata": {},
   "source": [
    "return_tensors (`str` or [`~file_utils.TensorType`], *optional*):\n",
    "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
    "\n",
    "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
    "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
    "        - `'np'`: Return Numpy `np.ndarray` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorh\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# tensorflow\n",
    "model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01607d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac52789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].transformer.layer.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.layers[0].transformer.layer.layers[0].variables:\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eea8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4abb511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1176d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pydot\n",
    "!pip install --upgrade graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc1d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ff050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb961de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "ab08db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"I went to the river bank.\", \n",
    "        \"I work at an investment bank.\", \n",
    "        \"Can we go to the bank?\"]\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "01142949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2253, 2000, 1996, 2314, 2924, 1012,  102],\n",
       "        [ 101, 1045, 2147, 2012, 2019, 5211, 2924, 1012,  102],\n",
       "        [ 101, 2064, 2057, 2175, 2000, 1996, 2924, 1029,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2be74",
   "metadata": {},
   "source": [
    "How does the tokenized text look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "78134be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'went', 'to', 'the', 'river', 'bank', '.', '[SEP]']\n",
      "['[CLS]', 'i', 'work', 'at', 'an', 'investment', 'bank', '.', '[SEP]']\n",
      "['[CLS]', 'can', 'we', 'go', 'to', 'the', 'bank', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "6e903ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "9\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(len(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4afec9",
   "metadata": {},
   "source": [
    "How is the output stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "867235b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "6e9c318d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.0416, -0.0590, -0.0143,  ...,  0.0987,  0.2583,  0.3616],\n",
       "         [ 0.4119, -0.1251, -0.0900,  ..., -0.2270,  0.4574,  0.1204],\n",
       "         [-0.1136, -0.3707,  0.0933,  ..., -0.0712, -0.2049,  0.1402],\n",
       "         ...,\n",
       "         [ 0.2677, -0.0853, -0.0798,  ..., -0.2010, -0.4547, -0.1793],\n",
       "         [ 0.3108, -0.2082, -0.3871,  ...,  0.1783, -0.0147, -0.4415],\n",
       "         [ 0.8262,  0.2993, -0.2272,  ...,  0.0407, -0.3076, -0.4611]],\n",
       "\n",
       "        [[ 0.1690, -0.0159, -0.0666,  ..., -0.1088,  0.4084,  0.2130],\n",
       "         [ 0.6823,  0.0927, -0.2938,  ..., -0.3990,  0.6445,  0.0726],\n",
       "         [ 0.4919,  0.4326,  0.1071,  ..., -0.6932,  0.2243, -0.2963],\n",
       "         ...,\n",
       "         [ 0.2798, -0.1044, -0.1349,  ..., -0.2584,  0.0653, -0.2844],\n",
       "         [ 0.3263, -0.5069, -0.4628,  ...,  0.1052,  0.1265, -0.6579],\n",
       "         [ 0.7799,  0.0762, -0.3130,  ...,  0.0021, -0.3228, -0.5257]],\n",
       "\n",
       "        [[ 0.2355, -0.0308,  0.0814,  ...,  0.0441,  0.3587,  0.2963],\n",
       "         [ 0.9666, -0.3159,  0.1571,  ..., -0.3292,  0.7363,  0.3653],\n",
       "         [ 0.5217, -0.2102, -0.0997,  ..., -0.1668,  0.5816, -0.1462],\n",
       "         ...,\n",
       "         [ 0.5715, -0.1523, -0.1739,  ..., -0.0636,  0.0537, -0.0734],\n",
       "         [ 0.2555, -0.6110, -0.3855,  ..., -0.2951,  0.2690,  0.1939],\n",
       "         [ 1.1065,  0.2672, -0.2597,  ...,  0.0451, -0.3290, -0.3413]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "134c59d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 768])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "bfcd7029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 768])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "f3800fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 768])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "66d5963c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 768])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6736561",
   "metadata": {},
   "source": [
    "Words with multiple meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "e33cd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "aa665e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_river = output[0][0][6]\n",
    "bank_financial = output[0][1][6]\n",
    "bank_universal = output[0][2][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "fb76120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_matrix = np.concatenate((bank_river.detach().numpy().reshape(1, 768), \n",
    "                              bank_financial.detach().numpy().reshape(1, 768), \n",
    "                              bank_universal.detach().numpy().reshape(1, 768)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "5f61a538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>River</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Universal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>River</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708200</td>\n",
       "      <td>0.775612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Investment</th>\n",
       "      <td>0.708200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Universal</th>\n",
       "      <td>0.775612</td>\n",
       "      <td>0.839302</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               River  Investment  Universal\n",
       "River       1.000000    0.708200   0.775612\n",
       "Investment  0.708200    1.000000   0.839302\n",
       "Universal   0.775612    0.839302   1.000000"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(cosine_similarity(bank_matrix), \n",
    "             columns=['River', 'Investment', 'Universal'],\n",
    "             index=['River', 'Investment', 'Universal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584627f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ba79ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'my', 'date', 'went', 'great', 'last', 'night', '!', '[SEP]', '[PAD]']\n",
      "['[CLS]', 'what', \"'\", 's', 'today', \"'\", 's', 'date', '?', '[SEP]']\n",
      "['[CLS]', 'this', 'date', 'is', 'too', 'sour', 'to', 'eat', '.', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Calendar</th>\n",
       "      <th>Food</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Relationship</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825439</td>\n",
       "      <td>0.743567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calendar</th>\n",
       "      <td>0.825439</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td>0.743567</td>\n",
       "      <td>0.771033</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Relationship  Calendar      Food\n",
       "Relationship      1.000000  0.825439  0.743567\n",
       "Calendar          0.825439  1.000000  0.771033\n",
       "Food              0.743567  0.771033  1.000000"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"My date went great last night!\", \n",
    "        \"What's today's date?\", \n",
    "        \"This date is too sour to eat.\"]\n",
    "encoded_input = tokenizer(text, return_tensors='pt', padding = True)\n",
    "\n",
    "for i in range(encoded_input['input_ids'].shape[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][i])) \n",
    "    \n",
    "output = model(**encoded_input)\n",
    "\n",
    "date_rel = output[0][0][2]\n",
    "date_time = output[0][1][7]\n",
    "date_food = output[0][2][2]\n",
    "\n",
    "date_matrix = np.concatenate((date_rel.detach().numpy().reshape(1, 768), \n",
    "                              date_time.detach().numpy().reshape(1, 768), \n",
    "                              date_food.detach().numpy().reshape(1, 768)))\n",
    "\n",
    "pd.DataFrame(cosine_similarity(date_matrix), \n",
    "             columns=['Relationship', 'Calendar', 'Food'],\n",
    "             index=['Relationship', 'Calendar', 'Food'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b756d22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0fd5287",
   "metadata": {},
   "source": [
    "## 3. Transfer learning without fine tuning - sentiment classification\n",
    "\n",
    "<img src='https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification-example.png' width=800>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "47678127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# already loaded\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "be7fbc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  a stirring , funny and finally transporting re...      1\n",
       "1  apparently reassembled from the cutting room f...      0\n",
       "2  they presume their audience wo n't sit still f...      0\n",
       "3  this is a visually stunning rumination on love...      1\n",
       "4  jonathan parker 's bartleby should have been t...      1"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', \n",
    "                 delimiter='\\t', header=None)\n",
    "df.columns = ['review', 'label']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c75850",
   "metadata": {},
   "source": [
    "#### 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "8ee8daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "tokenized = df['review'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "be1b57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 67\n",
      "Median length: 22.0\n",
      "Mean length: 23.341907514450867\n"
     ]
    }
   ],
   "source": [
    "print('Max length:', tokenized.map(len).max())\n",
    "print('Median length:', tokenized.map(len).median())\n",
    "print('Mean length:', tokenized.map(len).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c882e4d",
   "metadata": {},
   "source": [
    "Use tokenizer function that creates padded embeddings and outputs attention masks (what to consider, what not to consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "61a77dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 30\n",
    "\n",
    "def bert_tokenizer(text):\n",
    "    \n",
    "    encoded_text = tokenizer.encode_plus(text,  max_length = MAX_LEN, truncation=True,  padding='max_length',  \n",
    "                                         return_attention_mask=True, return_tensors='pt')\n",
    "    \n",
    "    return encoded_text['input_ids'][0].numpy(), encoded_text['attention_mask'][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "a539c15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  101,  7099,  3231,  2008,  2097,  2022, 20633,   102,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0], dtype=int64),\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=int64))"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer('Sample test that will be padded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "5414f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenized_padded, attention_masks = zip(*df['review'].apply(lambda x: bert_tokenizer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "4ca289e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6920, 30])\n",
      "torch.Size([6920, 30])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenized_padded)  \n",
    "attention_mask = torch.tensor(attention_masks)\n",
    "\n",
    "print(input_ids.shape)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b6199",
   "metadata": {},
   "source": [
    "#### 2. Apply BERT on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "8ebc4ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with torch.no_grad(): # no need to keep track of gradients\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "18fd9664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6920, 30, 768])"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5400989",
   "metadata": {},
   "source": [
    "6920 sentences, 30 words (tokens) in each sentence, 768 dimensions for each word (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351a115",
   "metadata": {},
   "source": [
    "#### 3. Get sentence embeddings out of the resuling tensor\n",
    "\n",
    "\n",
    "<img src='https://camo.githubusercontent.com/6c2185c7620a3fe52f1968752febb6467723f4485c257442d3b0ed03bb0da197/68747470733a2f2f6a616c616d6d61722e6769746875622e696f2f696d616765732f64697374696c424552542f626572742d6f75747075742d74656e736f722d73656c656374696f6e2e706e67' width=1000>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "55e6f07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6920, 768)\n",
      "(6920,)\n"
     ]
    }
   ],
   "source": [
    "X = last_hidden_states[0][:,0,:].numpy()\n",
    "y = df['label']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffac6d3",
   "metadata": {},
   "source": [
    "#### 4. Fit model, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "a066f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "4695acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 91, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "4353dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(max_iter = 1000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "790a5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = logit.predict(X_test)\n",
    "y_pred_prob = logit.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "7dfc759d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive class: 0.5216763005780347\n",
      "Accuracy: 0.8345375722543352\n",
      "AUC: 0.9162513712584235\n"
     ]
    }
   ],
   "source": [
    "print('Ratio of positive class:', y.value_counts()[1] / df.shape[0])\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred_class))\n",
    "print('AUC:', roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aad4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e7a140",
   "metadata": {},
   "source": [
    "#### 5. Predict sentiment of any text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "c35f9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    \n",
    "    _tokenized, _attention_mask = bert_tokenizer(text)\n",
    "\n",
    "    _tokenized = torch.reshape(torch.from_numpy(_tokenized), (1, 30))\n",
    "    _attention_mask = torch.reshape(torch.from_numpy(_attention_mask), (1, 30))\n",
    "    _last_hidden_state = model(_tokenized, attention_mask = _attention_mask)\n",
    "    _X = _last_hidden_state[0][:,0,:][0].detach().numpy().reshape(1, -1)\n",
    "\n",
    "    #predicted_class = logit.predict(_X)[0]\n",
    "    predicted_proba = logit.predict_proba(_X)[:, 1][0]\n",
    "\n",
    "    return print('Probability of being positive:', predicted_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "9ed2dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.4578798726886869\n"
     ]
    }
   ],
   "source": [
    "text = 'I though the movie was going to suck, but actually it turned out to be really good.'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "88518dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.21166203911710665\n"
     ]
    }
   ],
   "source": [
    "text = 'Overall OK, nothing special'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "a2abcc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.8485814533066036\n"
     ]
    }
   ],
   "source": [
    "text = 'Liked it'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "ce9bd3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6148351310661822\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "2de94f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being positive: 0.6895257016039957\n"
     ]
    }
   ],
   "source": [
    "text = 'What a fucking amazing picture!'\n",
    "predict_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0043f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0d998a2",
   "metadata": {},
   "source": [
    "## 4. Fine tuning a DistilBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7401dc22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11940307",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

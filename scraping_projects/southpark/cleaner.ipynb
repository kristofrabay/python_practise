{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of South Park transcripts for analysis\n",
    "\n",
    "<img src=\"https://forbes.hu/wp-content/uploads/2021/08/southpark2_forbes.jpg\" width=\"1000\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the following packages: pandas, numpy, matplotlib, seaborn, plotly, warnings\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../../')\n",
    "from src.import_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>line</th>\n",
       "      <th>season</th>\n",
       "      <th>episode_num</th>\n",
       "      <th>episode_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The boys</td>\n",
       "      <td>School days, school days, teacher's golden ru...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman Gets an Anal Probe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kyle Broflovski</td>\n",
       "      <td>Ah, damn it! My little brother's trying to fol...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman Gets an Anal Probe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ike Broflovski</td>\n",
       "      <td>Eat banana.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman Gets an Anal Probe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           speaker                                               line  season  \\\n",
       "0         The boys   School days, school days, teacher's golden ru...       1   \n",
       "1  Kyle Broflovski  Ah, damn it! My little brother's trying to fol...       1   \n",
       "2   Ike Broflovski                                        Eat banana.       1   \n",
       "\n",
       "   episode_num               episode_title  \n",
       "0            1  Cartman Gets an Anal Probe  \n",
       "1            1  Cartman Gets an Anal Probe  \n",
       "2            1  Cartman Gets an Anal Probe  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/transcript_scraped.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84229, 5)\n",
      "(79769, 5)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "\n",
    "data = data[~data['speaker'].str.lower().str.contains('\\d')]\n",
    "data = data[~data['speaker'].str.lower().str.contains(',')]\n",
    "data = data[~data['speaker'].isin(['Man', 'Woman'])]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'Eric Cartman': 'Cartman', 'Cartman:': 'Cartman', 'New Cartman': 'Cartman', 'Liane Cartman': 'Liane', 'Kyle.': 'Kyle',\n",
    "'Kyle:': 'Kyle', 'Kyle Broflovski': 'Kyle', 'Kyle (voiceover)': 'Kyle', 'Sheila Broflovski': 'Sheila', 'Gerald Broflovski': 'Gerald',\n",
    "'Stan:': 'Stan', 'stan': 'Stan', 'Stan Marsh': 'Stan', 'Randy Marsh': 'Randy', 'Shelly Marsh': 'Shelly', 'Sharon Marsh': 'Sharon',\n",
    "'\"Kenny\"': 'Kenny', 'Stuart McCormick': 'Stuart', 'Carol McCormick': 'Carol', 'Kenny McCormick': 'Kenny', 'Mrs. McCormick': 'Carol',\n",
    "'Ms. McCormick': 'Carol', 'Garrison' : 'Mr. Garrison', 'Mr Garrison' : 'Mr. Garrison', 'Herbert Garrison' : 'Mr. Garrison', \n",
    "'Mr. Garrison:' : 'Mr. Garrison', 'Mackey' : 'Mr. Mackey', 'Mr Mackey' : 'Mr. Mackey', 'Mr.Mackey' : 'Mr. Mackey',\n",
    "'Doctor' : 'Dr. Doctor', 'Dr. Mephesto' : 'Mephesto',\n",
    "'Pricipal Victoria' : 'Principal Victoria', 'Victoria' : 'Principal Victoria', 'Barbrady' : 'Officer Barbrady', 'Ike Broflovski' : 'Ike',\n",
    "'Mayor' : 'Mayor McDaniels', 'Mayor McDanniels' : 'Mayor McDaniels', 'Mayor McDaniels.' : 'Mayor McDaniels', 'Mayor McDaneils' : 'Mayor McDaniels'}\n",
    "\n",
    "data['speaker'].replace(mapper, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data['speaker'].str.lower().str.contains('man')]['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['speaking_to'] = data.groupby('episode_title')['speaker'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_count = data['speaker'].value_counts()\n",
    "drop_speakers = line_count[line_count <= 300].index.tolist()\n",
    "\n",
    "data.loc[data['speaker'].isin(drop_speakers), 'speaker'] = 'Other'\n",
    "data.loc[data['speaking_to'].isin(drop_speakers), 'speaking_to'] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That reduced all speakers to the ~top30 + an 'Other' category containing everybody else (won't use for analysis, modeling, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text\n",
    "\n",
    "Clean from special characters, narrating sentences, numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "ADDITIONAL_STOP_WORDS = ['hello', 'hi', 'oh', 'get', 'yeah', 'well', 'like', 'gonna', 'let', 'okay', 'people', 'u', 'uh', 'hey', 'would', 'got', 'one', 'going', 'know', 'right', 'go', 'come']\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "cList = {\n",
    "    \"ain't\": \"am not\",    \"aren't\": \"are not\",    \"can't've\": \"cannot have\",    \"can't\": \"cannot\",    \"'cause\": \"because\",    \"could've\": \"could have\",    \"couldn't've\": \"could not have\",\n",
    "    \"couldn't\": \"could not\",    \"didn't\": \"did not\",    \"doesn't\": \"does not\",    \"don't\": \"do not\",    \"hadn't've\": \"had not have\",    \"hadn't\": \"had not\",        \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",    \"he'd've\": \"he would have\",    \"he'd\": \"he would\",      \"he'll've\": \"he will have\",    \"he'll\": \"he will\",        \"he's\": \"he is\",    \"how'd'y\": \"how do you\",\n",
    "    \"how'd\": \"how did\",      \"how'll\": \"how will\",    \"how's\": \"how is\",    \"i'd've\": \"i would have\",    \"i'd\": \"i would\",    \"i'll've\": \"i will have\",    \"i'll\": \"i will\",    \n",
    "    \"i'm\": \"i am\",    \"i've\": \"i have\",    \"isn't\": \"is not\",    \"it'd've\": \"it would have\",    \"it'd\": \"it would\",    \"it'll've\": \"it will have\",      \"it'll\": \"it will\",      \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",    \"ma'am\": \"madam\",    \"mayn't\": \"may not\",    \"might've\": \"might have\",    \"mightn't've\": \"might not have\",    \"mightn't\": \"might not\",        \"must've\": \"must have\",\n",
    "    \"mustn't've\": \"must not have\",    \"mustn't\": \"must not\",    \"needn't've\": \"need not have\",      \"needn't\": \"need not\",      \"oughtn't've\": \"ought not have\",    \"oughtn't\": \"ought not\",\n",
    "    \"shan't've\": \"shall not have\",    \"shan't\": \"shall not\",    \"sha'n't\": \"shall not\",    \"she'd've\": \"she would have\",    \"she'd\": \"she would\",    \"she'll've\": \"she will have\",\n",
    "    \"she'll\": \"she will\",      \"she's\": \"she is\",    \"should've\": \"should have\",    \"shouldn't've\": \"should not have\",    \"shouldn't\": \"should not\",      \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",    \"that'd've\": \"that would have\",    \"that'd\": \"that would\",      \"that's\": \"that is\",    \"there'd've\": \"there would have\",    \"there'd\": \"there had\",  \n",
    "    \"there's\": \"there is\",    \"they'd've\": \"they would have\",    \"they'd\": \"they would\",    \"they'll've\": \"they will have\",    \"they'll\": \"they will\",      \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",    \"to've\": \"to have\",    \"wasn't\": \"was not\",    \"we'd've\": \"we would have\",    \"we'd\": \"we would\",    \"we'll've\": \"we will have\",      \"we'll\": \"we will\",  \n",
    "    \"we're\": \"we are\",    \"we've\": \"we have\",    \"weren't\": \"were not\",    \"what'll've\": \"what will have\",    \"what'll\": \"what will\",        \"what're\": \"what are\",    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",    \"when's\": \"when is\",    \"when've\": \"when have\",    \"where'd\": \"where did\",    \"where's\": \"where is\",    \"where've\": \"where have\",    \"who'll've\": \"who will have\",\n",
    "    \"who'll\": \"who will\",        \"who's\": \"who is\",    \"who've\": \"who have\",    \"why's\": \"why is\",    \"why've\": \"why have\",    \"will've\": \"will have\",    \"won't've\": \"will not have\",\n",
    "    \"won't\": \"will not\",        \"would've\": \"would have\",    \"wouldn't've\": \"would not have\",    \"wouldn't\": \"would not\",    \"y'all'd've\": \"you all would have\",    \"y'all'd\": \"you all would\",\n",
    "    \"y'all're\": \"you all are\",    \"y'all've\": \"you all have\",    \"y'all\": \"you all\",    \"y'alls\": \"you all\",      \"you'd've\": \"you would have\",    \"you'd\": \"you had\",\n",
    "    \"you'll've\": \"you will have\",    \"you'll\": \"you will\",      \"you're\": \"you are\",    \"you've\": \"you have\" }\n",
    "\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "def text_cleaner(text, treat_contractions = True, remove_stopwords = True, lemmatize = True):  \n",
    "    \n",
    "    # lower chars and treat whitespaces\n",
    "    text = text.lower()\n",
    "    text = re.compile('\\s+').sub(' ', text) # whitespaces\n",
    "    text = text.strip() # trailing whitespaces  \n",
    "\n",
    "    # delete narration part between [ and ]\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "\n",
    "    # contractions\n",
    "    if treat_contractions:\n",
    "        text = decontracted(text)\n",
    "        text = ' '.join([cList.get(word,word) for word in text.split()])\n",
    "\n",
    "    # delete special characters and numbers\n",
    "    text = re.compile('[^a-z]').sub(' ', text) \n",
    "\n",
    "    # after numbers\n",
    "    text = re.sub(' th ', '', text)\n",
    "    text = re.sub(' st ', '', text)\n",
    "    text = re.sub(' nd ', '', text)\n",
    "    text = re.compile('\\s+').sub(' ', text) # whitespaces\n",
    "    text = re.compile('\\s+').sub(' ', text) # whitespaces\n",
    "    text = text.strip() # trailing whitespaces \n",
    "\n",
    "    # stopwords\n",
    "    if remove_stopwords:        \n",
    "        text = ' '.join([word for word in text.split() if word not in STOP_WORDS + ADDITIONAL_STOP_WORDS])\n",
    "\n",
    "    # lemmatize   \n",
    "    if lemmatize:\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "    # stopwords after lemmatization\n",
    "    if remove_stopwords:        \n",
    "        text = ' '.join([word for word in text.split() if word not in STOP_WORDS + ADDITIONAL_STOP_WORDS])\n",
    "\n",
    "    # final whitespace treatment\n",
    "    text = re.compile('\\s+').sub(' ', text) # whitespaces\n",
    "    text = text.strip() # trailing whitespaces     \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data['line_clean'] = data['line'].apply(lambda x: text_cleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['line_clean'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check word count for more possible stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in text: 23116\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(data['line_clean'])\n",
    "counter = Counter(ngrams(all_text.split(' '), 1))\n",
    "\n",
    "print('Unique words in text:', len(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(counter, orient = 'index')\\\n",
    "#     .reset_index()\\\n",
    "#     .sort_values(0, ascending = False)\\\n",
    "#     .reset_index(drop = True)\\\n",
    "#     .rename(columns = {'index' : 'token', 0 : 'count'}).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/transcript_cleaned.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1c82bdd170d252fe165a9bec4d731e9ac025b41ad0f7b30ff5fda4e58991de2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
